@article{BergenDuenkAlbersBijmaEtAl2020,
  author    = {Giel H. H. van Bergen and P. Duenk and C. A. Albers and P. Bijma and M. Calus and Y. Wientjes and H. Kappen},
  journal   = {Genetics Selection Evolution},
  title     = {Bayesian neural networks with variable selection for prediction of genotypic values},
  year      = {2020},
  month     = {may},
  number    = {1},
  volume    = {52},
  doi       = {10.1186/s12711-020-00544-8},
  pmid      = {32414320},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://www.semanticscholar.org/paper/48b88734145b67b9204446be1ab5a42a34b7e0f9},
  venue     = {Genetics Selection Evolution}
}

@article{Bonabeau2002,
  author    = {E. Bonabeau},
  journal   = {PNAS},
  title     = {Agent-based modeling: Methods and techniques for simulating human systems},
  year      = {2002},
  month     = {may},
  number    = {suppl{\_}3},
  pages     = {7280--7287},
  volume    = {99},
  abstract  = {Agent-based modeling is a powerful simulation modeling technique that has seen a number of applications in the last few years, including applications to real-world business problems. After the basic principles of agent-based simulation are briefly introduced, its four areas of application are discussed by using real-world applications: flow simulation, organizational simulation, market simulation, and diffusion simulation. For each category, one or several business applications are described and analyzed.},
  doi       = {10.1073/PNAS.082080899},
  pmid      = {12011407},
  publisher = {Proceedings of the National Academy of Sciences},
  url       = {https://www.semanticscholar.org/paper/ca94bffc4dce4b55f92555b0536f0f823ed35989},
  venue     = {Proceedings of the National Academy of Sciences of the United States of America}
}

@article{ChenGaoLiangWang2020,
  author    = {Yao Chen and Qingyi Gao and F. Liang and Xiao Wang},
  journal   = {Journal of Computational and Graphical Statistics},
  title     = {Nonlinear Variable Selection via Deep Neural Networks},
  year      = {2020},
  month     = {oct},
  number    = {2},
  pages     = {484--492},
  volume    = {30},
  abstract  = {Abstract This article presents a general framework for high-dimensional nonlinear variable selection using deep neural networks under the framework of supervised learning. The network architecture includes both a selection layer and approximation layers. The problem can be cast as a sparsity-constrained optimization with a sparse parameter in the selection layer and other parameters in the approximation layers. This problem is challenging due to the sparse constraint and the nonconvex optimization. We propose a novel algorithm, called deep feature selection, to estimate both the sparse parameter and the other parameters. Theoretically, we establish the algorithm convergence and the selection consistency when the objective function has a generalized stable restricted Hessian. This result provides theoretical justifications of our method and generalizes known results for high-dimensional linear variable selection. Simulations and real data analysis are conducted to demonstrate the superior performance of our method. Supplementary materials for this article are available online.},
  doi       = {10.1080/10618600.2020.1814305},
  publisher = {Informa {UK} Limited},
  url       = {https://www.semanticscholar.org/paper/f7dbad3e50781a35034edeb2a232f77000903eac},
  venue     = {Journal of Computational And Graphical Statistics}
}

@article{KassaniLuGuenBelloyEtAl2021,
  author        = {P. H. Kassani and Fred Lu and Yann Le Guen and Michaël E. Belloy and Zihuai He},
  title         = {Deep neural networks with controlled variable selection for the identification of putative causal genetic variants},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.1038/s42256-022-00525-0},
  eprint        = {2109.14719},
  url           = {https://www.semanticscholar.org/paper/159d9ba41519d573e7ad7fc879d2834288c05f2a},
  venue         = {Nature Machine Intelligence}
}

@article{KimShinKimHeo2020,
  author    = {Taereem Kim and Ju-young Shin and Hanbeen Kim and J. Heo},
  journal   = {Water Resources Research},
  title     = {Ensemble‐Based Neural Network Modeling for Hydrologic Forecasts: Addressing Uncertainty in the Model Structure and Input Variable Selection},
  year      = {2020},
  month     = {jun},
  number    = {6},
  volume    = {56},
  abstract  = {Artificial neural networks (ANNs) have been extensively used to forecast monthly precipitation for water resources management over the past few decades. Efforts to produce more accurate and stable forecasts face ongoing challenges as the so‐called single‐ANN (S‐ANN) approach has several limitations, particularly regarding uncertainty. Many attempts have been made to deal with different types of uncertainties by applying ensemble approaches. Here, we propose a new ANN ensemble model (ANN‐ENS) dealing with uncertainty in model structure and input variable selection to provide a more accurate and stable forecasting performance. This model is structured by generating various input layers, considering all the candidate input variables (i.e.,large‐scale climate indices and lagged precipitation). We developed a modified backward elimination method to select the preliminary input variables from all the candidate input variables. Then, we tested and validated the proposed ANN‐ENS using observed monthly precipitation from 10 meteorological stations in the Han River basin, South Korea. Our results demonstrated that the ANN‐ENS enhanced the forecasting performance in terms of both accuracy and stability. Although a significant uncertainty was introduced by using all the candidate input variables, the forecasting result outperformed S‐ANNs for all employed stations. Additionally, the ANN‐ENS provided a more stable forecasting performance in comparison with S‐ANNs, which are highly sensitive. Moreover, the generated ensemble members were slightly biased at some stations but were generally reliable.},
  doi       = {10.1029/2019WR026262},
  publisher = {American Geophysical Union ({AGU})},
  url       = {https://www.semanticscholar.org/paper/b5987345add152c94940cbf1ddb0bc2e97a76044},
  venue     = {Water Resources Research}
}

@book{KubatKubat2017,
  title     = {An introduction to machine learning},
  author    = {Kubat, M. and Kubat, J.A.},
  volume    = {2},
  year      = {2017},
  edition   = {Third},
  publisher = {Springer}
}

@article{LiBarneyHudacNuechterleinEtAl2020,
  author   = {B. Li and E. Barney and C. Hudac and Nicholas Nuechterlein and P. Ventola and Linda G. Shapiro and F. Shic},
  title    = {Selection of Eye-Tracking Stimuli for Prediction by Sparsely Grouped Input Variables for Neural Networks: towards Biomarker Refinement for Autism},
  year     = {2020},
  abstract = {Eye tracking has become a powerful tool in the study of autism spectrum disorder (ASD). Current, large-scale efforts aim to identify specific eye-tracking stimuli to be used as biomarkers for ASD, with the intention of informing the diagnostic process, monitoring therapeutic response, predicting outcomes, or identifying subgroups with the spectrum. However, there are hundreds of candidate experimental paradigms, each of which contains dozens or even hundreds of individual stimuli. Each stimuli is associated with an array of potential derived outcome variables, thus the number of variables to consider can be enormous. Standard variable selection techniques are not applicable to this problem, because selection must be done at the level of stimuli and not individual variables. In other words, this is a grouped variable selection problem. In this work, we apply lasso, group lasso, and a new technique, Sparsely Grouped Input Variables for Neural Network (SGIN), to select experimental stimuli for group discrimination and regression with clinical variables. Using a dataset obtained from children with and without ASD who were administered a battery containing 109 different stimuli presentations involving 9647 features, we are able to retain strong group separation even with only 11 out of the 109 stimuli. This work sets the stage for concerted techniques designed around engines to iteratively refine and define next-generation biomarkers using eye tracking for psychiatric conditions. http://github.com/beibinli/SGIN},
  doi      = {10.1145/3379155.3391334},
  url      = {https://www.semanticscholar.org/paper/464aea9b06380e292bc53c3c158e1e67d2b369b5},
  venue    = {Eye Tracking Research & Application}
}

@inbook{MayDandyMaier2011,
  author    = {R. J. May and G. Dandy and H. Maier},
  publisher = {{InTech}},
  title     = {Review of Input Variable Selection Methods for Artificial Neural Networks},
  year      = {2011},
  month     = {Apr},
  abstract  = {The choice of input variables is a fundamental, and yet crucial consideration in identifying the optimal functional form of statistical models. The task of selecting input variables is common to the development of all statistical models, and is largely dependent on the discovery of relationships within the available data to identify suitable predictors of the model output. In the case of parametric, or semi-parametric empirical models, the difficulty of the input variable selection task is somewhat alleviated by the a priori assumption of the functional form of the model, which is based on some physical interpretation of the underlying system or process being modelled. However, in the case of artificial neural networks (ANNs), and other similarly data-driven statistical modelling approaches, there is no such assumption made regarding the structure of the model. Instead, the input variables are selected from the available data, and the model is developed subsequently. The difficulty of selecting input variables arises due to (i) the number of available variables, which may be very large; (ii) correlations between potential input variables, which creates redundancy; and (iii) variables that have little or no predictive power. Variable subset selection has been a longstanding issue in fields of applied statistics dealing with inference and linear regression (Miller, 1984), and the advent of ANN models has only served to create new challenges in this field. The non-linearity, inherent complexity and non-parametric nature of ANN regression make it difficult to apply many existing analytical variable selection methods. The difficulty of selecting input variables is further exacerbated during ANN development, since the task of selecting inputs is often delegated to the ANN during the learning phase of development. A popular notion is that an ANN is adequately capable of identifying redundant and noise variables during training, and that the trained network will use only the salient input variables. ANN architectures can be built with arbitrary flexibility and can be successfully trained using any combination of input variables (assuming they are good predictors). Consequently, allowances are often made for a large number of input variables, with the belief that the ability to incorporate such flexibility and redundancy creates a more robust model. Such pragmatism is perhaps symptomatic of the popularisation of ANN models through machine learning, rather than statistical learning theory. ANN models are too often developed without due consideration given to the effect that the choice of input variables has on model complexity, learning difficulty, and performance of the subsequently trained ANN. 1},
  booktitle = {Artificial Neural Networks - Methodological Advances and Biomedical Applications},
  doi       = {10.5772/16004},
  url       = {https://www.semanticscholar.org/paper/977f9694f10f25a499e2d32f660da85e4ca183de}
}

@article{Newman2003,
  author        = {M. Newman},
  journal       = {{SIAM} Review},
  title         = {The Structure and Function of Complex Networks},
  year          = {2003},
  month         = {jan},
  number        = {2},
  pages         = {167--256},
  volume        = {45},
  abstract      = {Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
  archiveprefix = {arXiv},
  doi           = {10.1137/S003614450342480},
  eprint        = {cond-mat/0303516},
  publisher     = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  url           = {https://www.semanticscholar.org/paper/e6c4925fb114d13a8568f88957c167c928f0c9f1},
  venue         = {SIAM Review}
}

@book{Newman2010,
  author    = {M. Newman},
  publisher = {Oxford University Press},
  title     = {Networks: An Introduction},
  year      = {2010},
  OPTmonth  = {Mar},
  abstract  = {The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks.The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together for the first time the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.},
  doi       = {10.1093/ACPROF:OSO/9780199206650.001.0001},
  url       = {https://www.semanticscholar.org/paper/effa683ec6111aaf4b4d29ab2fb26b832844d9e1}
}

@article{SunHuangWongJang2017,
  author   = {Kai Sun and Shao-Hsuan Huang and D. Wong and Shi-Shang Jang},
  title    = {Design and Application of a Variable Selection Method for Multilayer Perceptron Neural Network With LASSO},
  year     = {2017},
  abstract = {In this paper, a novel variable selection method for neural network that can be applied to describe nonlinear industrial processes is developed. The proposed method is an iterative two-step approach. First, a multilayer perceptron is constructed. Second, the least absolute shrinkage and selection operator is introduced to select the input variables that are truly essential to the model with the shrinkage parameter is determined using a cross-validation method. Then, variables whose input weights are zero are eliminated from the data set. The algorithm is repeated until there is no improvement in the model accuracy. Simulation examples as well as an industrial application in a crude distillation unit are used to validate the proposed algorithm. The results show that the proposed approach can be used to construct a more compressed model, which incorporates a higher level of prediction accuracy than other existing methods.},
  doi      = {10.1109/TNNLS.2016.2542866},
  pmid     = {28113826},
  url      = {https://www.semanticscholar.org/paper/b6b304deeac552713333c0aaa625d04cd18fb37f},
  venue    = {IEEE Transactions on Neural Networks and Learning Systems}
}

@Comment{jabref-meta: databaseType:bibtex;}
